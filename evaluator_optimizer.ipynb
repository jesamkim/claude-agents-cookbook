{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator-Optimizer Workflow\n",
    "이 워크플로우에서는 하나의 LLM이 응답을 생성하고 다른 LLM이 순환적으로 평가와 피드백을 제공합니다.\n",
    "\n",
    "### 이 워크플로우의 적용 시점\n",
    "이 워크플로우는 다음과 같은 상황에서 특히 효과적입니다:\n",
    "\n",
    "- 명확한 평가 기준이 있는 경우\n",
    "- 반복적인 개선이 가치를 창출하는 경우\n",
    "\n",
    "적합성을 판단하는 두 가지 지표는 다음과 같습니다:\n",
    "\n",
    "- 피드백이 제공될 때 LLM 응답이 입증 가능한 수준으로 개선되는 경우\n",
    "- LLM이 스스로 의미 있는 피드백을 제공할 수 있는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from util import llm_call, extract_xml\n",
    "\n",
    "def generate(prompt: str, task: str, context: str = \"\") -> tuple[str, str]:\n",
    "    \"\"\"Generate and improve a solution based on feedback.\"\"\"\n",
    "    full_prompt = f\"{prompt}\\n{context}\\nTask: {task}\" if context else f\"{prompt}\\nTask: {task}\"\n",
    "    response = llm_call(full_prompt)\n",
    "    thoughts = extract_xml(response, \"thoughts\")\n",
    "    result = extract_xml(response, \"response\")\n",
    "    \n",
    "    print(\"\\n=== GENERATION START ===\")\n",
    "    print(f\"Thoughts:\\n{thoughts}\\n\")\n",
    "    print(f\"Generated:\\n{result}\")\n",
    "    print(\"=== GENERATION END ===\\n\")\n",
    "    \n",
    "    return thoughts, result\n",
    "\n",
    "def evaluate(prompt: str, content: str, task: str) -> tuple[str, str]:\n",
    "    \"\"\"Evaluate if a solution meets requirements.\"\"\"\n",
    "    full_prompt = f\"{prompt}\\nOriginal task: {task}\\nContent to evaluate: {content}\"\n",
    "    response = llm_call(full_prompt)\n",
    "    evaluation = extract_xml(response, \"evaluation\")\n",
    "    feedback = extract_xml(response, \"feedback\")\n",
    "    \n",
    "    print(\"=== EVALUATION START ===\")\n",
    "    print(f\"Status: {evaluation}\")\n",
    "    print(f\"Feedback: {feedback}\")\n",
    "    print(\"=== EVALUATION END ===\\n\")\n",
    "    \n",
    "    return evaluation, feedback\n",
    "\n",
    "def loop(task: str, evaluator_prompt: str, generator_prompt: str) -> tuple[str, list[dict]]:\n",
    "    \"\"\"Keep generating and evaluating until requirements are met.\"\"\"\n",
    "    memory = []\n",
    "    chain_of_thought = []\n",
    "    \n",
    "    thoughts, result = generate(generator_prompt, task)\n",
    "    memory.append(result)\n",
    "    chain_of_thought.append({\"thoughts\": thoughts, \"result\": result})\n",
    "    \n",
    "    while True:\n",
    "        evaluation, feedback = evaluate(evaluator_prompt, result, task)\n",
    "        if evaluation == \"PASS\":\n",
    "            return result, chain_of_thought\n",
    "            \n",
    "        context = \"\\n\".join([\n",
    "            \"Previous attempts:\",\n",
    "            *[f\"- {m}\" for m in memory],\n",
    "            f\"\\nFeedback: {feedback}\"\n",
    "        ])\n",
    "        \n",
    "        thoughts, result = generate(generator_prompt, task, context)\n",
    "        memory.append(result)\n",
    "        chain_of_thought.append({\"thoughts\": thoughts, \"result\": result})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예시 활용 사례: 반복적 코딩 루프\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GENERATION START ===\n",
      "Thoughts:\n",
      "\n",
      "상수 시간 O(1)으로 최소값을 얻으려면 별도의 최소값 스택이 필요합니다.\n",
      "주 스택과 최소값 스택을 동기화하여 관리하면 모든 연산을 O(1)에 수행할 수 있습니다.\n",
      "\n",
      "\n",
      "Generated:\n",
      "\n",
      "```python\n",
      "class MinStack:\n",
      "    def __init__(self):\n",
      "        self.stack = []        # 주 스택\n",
      "        self.minStack = []     # 최소값 스택\n",
      "        \n",
      "    def push(self, x: int) -> None:\n",
      "        self.stack.append(x)\n",
      "        # 최소값 스택이 비어있거나 새 값이 현재 최소값보다 작거나 같으면 추가\n",
      "        if not self.minStack or x <= self.minStack[-1]:\n",
      "            self.minStack.append(x)\n",
      "            \n",
      "    def pop(self) -> None:\n",
      "        if not self.stack:\n",
      "            return\n",
      "        # 제거되는 값이 현재 최소값이면 최소값 스택에서도 제거\n",
      "        if self.stack[-1] == self.minStack[-1]:\n",
      "            self.minStack.pop()\n",
      "        self.stack.pop()\n",
      "        \n",
      "    def getMin(self) -> int:\n",
      "        if not self.minStack:\n",
      "            return None\n",
      "        return self.minStack[-1]\n",
      "```\n",
      "\n",
      "=== GENERATION END ===\n",
      "\n",
      "=== EVALUATION START ===\n",
      "Status: PASS\n",
      "Feedback: \n",
      "코드가 모든 요구사항을 충족하며 잘 구현되어 있습니다:\n",
      "\n",
      "1. 정확성\n",
      "- 모든 요구 기능(push, pop, getMin)이 올바르게 구현됨\n",
      "- 엣지 케이스(빈 스택 등) 처리가 적절함\n",
      "\n",
      "2. 시간 복잡도\n",
      "- 모든 연산이 O(1) 시간 복잡도를 만족함\n",
      "- 추가 공간 복잡도는 O(n)이지만 이는 허용됨\n",
      "\n",
      "3. 스타일 및 모범 사례\n",
      "- 코드가 깔끔하고 가독성이 좋음\n",
      "- 적절한 주석 사용\n",
      "- 명확한 변수명 사용\n",
      "\n",
      "추가 개선사항 없음\n",
      "\n",
      "=== EVALUATION END ===\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('\\n```python\\nclass MinStack:\\n    def __init__(self):\\n        self.stack = []        # 주 스택\\n        self.minStack = []     # 최소값 스택\\n        \\n    def push(self, x: int) -> None:\\n        self.stack.append(x)\\n        # 최소값 스택이 비어있거나 새 값이 현재 최소값보다 작거나 같으면 추가\\n        if not self.minStack or x <= self.minStack[-1]:\\n            self.minStack.append(x)\\n            \\n    def pop(self) -> None:\\n        if not self.stack:\\n            return\\n        # 제거되는 값이 현재 최소값이면 최소값 스택에서도 제거\\n        if self.stack[-1] == self.minStack[-1]:\\n            self.minStack.pop()\\n        self.stack.pop()\\n        \\n    def getMin(self) -> int:\\n        if not self.minStack:\\n            return None\\n        return self.minStack[-1]\\n```\\n',\n",
       " [{'thoughts': '\\n상수 시간 O(1)으로 최소값을 얻으려면 별도의 최소값 스택이 필요합니다.\\n주 스택과 최소값 스택을 동기화하여 관리하면 모든 연산을 O(1)에 수행할 수 있습니다.\\n',\n",
       "   'result': '\\n```python\\nclass MinStack:\\n    def __init__(self):\\n        self.stack = []        # 주 스택\\n        self.minStack = []     # 최소값 스택\\n        \\n    def push(self, x: int) -> None:\\n        self.stack.append(x)\\n        # 최소값 스택이 비어있거나 새 값이 현재 최소값보다 작거나 같으면 추가\\n        if not self.minStack or x <= self.minStack[-1]:\\n            self.minStack.append(x)\\n            \\n    def pop(self) -> None:\\n        if not self.stack:\\n            return\\n        # 제거되는 값이 현재 최소값이면 최소값 스택에서도 제거\\n        if self.stack[-1] == self.minStack[-1]:\\n            self.minStack.pop()\\n        self.stack.pop()\\n        \\n    def getMin(self) -> int:\\n        if not self.minStack:\\n            return None\\n        return self.minStack[-1]\\n```\\n'}])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator_prompt = \"\"\"\n",
    "다음 코드 구현을 아래 기준으로 평가하세요:\n",
    "1. 코드 정확성\n",
    "2. 시간 복잡도\n",
    "3. 스타일 및 모범 사례\n",
    "\n",
    "평가만 진행하고 과제 해결을 시도하지 마세요.\n",
    "모든 기준이 충족되고 더 이상 개선 제안이 없는 경우에만 \"PASS\"를 출력하세요.\n",
    "평가를 다음 형식으로 간단히 출력하세요.\n",
    "\n",
    "<evaluation>PASS, NEEDS_IMPROVEMENT, 또는 FAIL</evaluation>\n",
    "<feedback>\n",
    "개선이 필요한 사항과 그 이유.\n",
    "</feedback>\n",
    "\"\"\"\n",
    "\n",
    "generator_prompt = \"\"\"\n",
    "<user input>에 기반하여 과제를 완료하는 것이 목표입니다. 이전 생성에 대한\n",
    "피드백이 있다면 이를 반영하여 해결책을 개선해야 합니다.\n",
    "\n",
    "답변을 다음 형식으로 간단히 출력하세요:\n",
    "\n",
    "<thoughts>\n",
    "[과제와 피드백에 대한 이해, 개선 계획]\n",
    "</thoughts>\n",
    "\n",
    "<response>\n",
    "[코드 구현 내용]\n",
    "</response>\n",
    "\"\"\"\n",
    "\n",
    "task = \"\"\"\n",
    "<user input>\n",
    "스택을 구현합니다:\n",
    "1. push(x)\n",
    "2. pop()\n",
    "3. getMin()\n",
    "모든 연산은 O(1)이어야 합니다.\n",
    "</user input>\n",
    "\"\"\"\n",
    "\n",
    "loop(task, evaluator_prompt, generator_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
